\documentclass[11pt]{article}
\usepackage{xltxtra}
\usepackage{polyglossia}
\setdefaultlanguage[spelling=modern]{russian}
%\setmainfont[Mapping=tex-text]{DejaVu Sans}
%\setmainfont[Mapping=tex-text]{Liberation Sans}
\setmonofont[Mapping=tex-text]{DejaVu Sans Mono}
\setmainfont[Mapping=tex-text]{Linux Libertine O}
%\setmonofont[Mapping=tex-text]{Liberation Mono}

\usepackage{verbatim}
\usepackage{tabularx}
\usepackage{float}
\usepackage{url}

\usepackage{indentfirst}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage[a4paper]{geometry}
\geometry{left=25mm}
\geometry{right=25mm}
\geometry{top=25mm}
\geometry{bottom=15mm}

\usepackage{listings}

\setlength{\parindent}{10mm}
\makeatletter
% Переопределение команды секции
\renewcommand{\section}{\@startsection{section}{1}%
{\parindent}{3.25ex plus 1ex minus .2ex}%
{1.5ex plus .2ex}{\bfseries\large\uppercase}}

% Переопределение команды подсекции
\renewcommand{\subsection}{\@startsection{subsection}{2}%
{\parindent}{3.25ex plus 1ex minus .2ex}%
{1.5ex plus .2ex}{\bfseries}}
\makeatother

%\newcommand{\msection}[1]{\section[#1]{#1}}
%\newcommand{\msection}[1]{\section[#1]{\uppercase{#1}}}

%opening
%\title{}
%\author{Филиппов Алексей Николаевич}

\newcommand{\titletext}{
\Large Лабораторная работа №2\\*
по дисциплине <<Методы организации нейронных сетей>>\\*
на тему <<Двухслойные сигмоиды>>
}

\newcommand{\myvariant}{16}
\newcommand{\mygroup}{6-78-11}

\newcommand{\includepicture}[3]{
\begin{figure}[H]
\begin{center}
\leavevmode
%\large{\textbf{#2}}
\includegraphics[width=#3\textwidth]{#1}
\end{center}
\caption{#2}
\end{figure}
}

\lstset{ %
language=C++,                   % the language of the code
basicstyle=\tiny,               % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=2,                   % the step between two line-numbers. If it's 1, each line
                                % will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
%backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                   % adds a frame around the code
tabsize=2,                      % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                % also try caption instead of title
escapeinside={\%*}{*)},         % if you want to add a comment within your code
morekeywords={*,...}            % if you want to add more keywords to the set
}

\begin{document}

\include{title}

\tableofcontents
\pagebreak

\section{Постановка задачи}

Аппроксимация функции $t = f(x)$. Обучение при помощи <<тепловой машины>> (стохастический метод).

\section{Двухслойные сигмоиды}

\subsection{Тепловая машина}
\begin{algorithm}[H]
    \caption{Обучение}
    \begin{algorithmic}
        \State Создать сеть с необходимой топологией.
        \State Установить значение весов случайными. В данной реализации $w \in [-5, 5]$.
        \Repeat
            \State Сформировать случайное изменение значений весов нейросети. В данной реализации $\Delta w \in [-0.25, 0.25]$.
            \State Рассчитать суммарную ошибку: $E^\Sigma = \sum\limits_{k=1}^K E^k = \sum\limits \frac{1}{2}\left(t^k-y^k\right)^2$.
            \If{суммарная ошибка $<$ лучший найденный результат}
                \State Сохранить изменение.
            \Else
                \State С шансом $P(\Delta E^\Sigma, T)$ сохранить изменение.
                    В данной реализации $T = \frac{T_0}{1+e}, P(\Delta E^\Sigma, T) = \frac{T^2}{T^2+\left(\Delta E^\Sigma\right)^2}$,
                    где $e$ -- номер итерации.
            \EndIf
        \Until{Условие остановки: суммарная ошибка не превосходит (успех) заданную величину либо исчерпан лимит итераций (провал).}
    \end{algorithmic}
\end{algorithm}

При снижении искусственной температуры T различают 3 этапа:
\begin{enumerate}
    \item Значение Т велико. Величина теплового движения также велика и практически компенсирует величину силу тяжести.
        Происходит хаотическое изменение синапсов без направленности к минимуму. Тепловая машина перегрета. Нахождение в этом этапе нужно сокращать.
    \item Значение T среднее. При этом величина теплового движения значительна, но меньше силы тяжести. Это полезный этап работы тепловой машины – направленный перебор.
    \item Значение T мало. При этом величина теплового движения незначительна, то есть в процессе обучения действует только сила тяжести.
        Это заключительный этап работы тепловой машины, он соответствует градиентному методу. Тепловая машина остыла.
        Нахождение в этом этапе нужно сокращать -- прекращение обучения или завершение его методом обратного распространения ошибки.
\end{enumerate}

\section{Архитектура приложения}
Приложение состоит из двух частей:
\begin{enumerate}
    \item Библиотека, реализующая примитивы для работы с нейронными сетями.
    \item Графический интерфейс пользователя.
\end{enumerate}

\section{Реализация стохастического метода обучения}
\lstinputlisting{include/StochasticSupervisor.hpp}
\lstinputlisting{src/StochasticSupervisor.cpp}

\section{Описание интерфейса}
\includepicture{TrainingDialogRunning}{Процесс обучения}{1}
\includepicture{TrainingDialogSuccess}{Обучение успешно завершено}{1}
\includepicture{Task21}{Результат обучения}{1}

\section{Тестовые примеры}
\includepicture{Task21}{Задание 1}{1}
\includepicture{Task22}{Задание 2}{1}

\section{Выводы}
В работе была использована двухслойная нейронная сеть с сигмоидальной функцией активации.
При помощи такой сети была реализована аппроксимация функций.
Основными проблемами в данной реализации являются скорость обучения и точность.
Скорость обучения зависит от выбранной точности. Более того, в некоторых случаях сеть
не может её достичь за заданное количество шагов. Это может говорить как о неверных
настройках сети (количество нейронов, топология), так и о недостаточном количестве шагов.

\end{document}
